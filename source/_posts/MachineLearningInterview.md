---
title: 机器学习基础
date: 2020-04-14 00:21:43
author: Kevin Feng
tags:
- 机器学习
- 面试
---
本文主要介绍一些传统机器学习与面试相关的概念
<!--more-->
## LR(逻辑回归)
### 特点
- 用于分类问题
- 使用似然估计
- 对数据偏差大的更敏感
### 公式
> `$Y = \sigma (\theta \times X)$`
- 就是矩阵乘法套了个sigmoid()
#### Loss
>  MSE: `$L = \frac{1}{2m}\Sigma_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$`
> CrossEntropy: `$L = -\frac{1}{m}\Sigma(ylog(h))+(1-y)log(1-h))$`

#### 反向传播:
> `$\sigma'(x) = \sigma(x)(1-\sigma(x)) $`
> `$\theta += \alpha \times \Delta \theta $`

## 线性回归
- 最小二乘法
- 直接求解不用似然估计
- 对所有数据敏感
## EM(极大似然估计)

## 决策树

## K-NN
- 选K个点作为邻居
- 他们投票作为结果
- 计算距离
- 小规模样本ok
- 大规模的计算就太麻烦了
- 不需要迭代
- 跟K-Means没关系
- 
## K-Means
- 聚类问题
- 跟K-NN没关系
- 随机选K个点先聚类然后再重新计算这些点,点是分类之后的点得中点
- 受初始化程度影响
- 多次重复效果更好

## 集成学习
### 特点
通过几个弱分类器组合成强分类器
### Stacking
- 组合不同种的分类器,外加嵌套一层分类器调整不同分类器的权重.
### Bagging
- 将训练集拆成几个子集,分别训练(又放回,可重复,但是抽样多次所以还是不同的子集)
- 最后投票
- 例子: 随机森林
- 可并行计算
### Boosting
- 将弱分类器通过加权组合成强分类器
- 不可并行计算,需要利用上一次的结果,类似RNN
#### AdaBoost
#### XGBoost

## 朴素贝叶斯
> `$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$`

## 算法之外的概念
### Bias/Variance
- Bias体现了模型整体情况,
- variance体现了模型在不同训练集中泛化能力
- B低V低,好模型
- B高V高,没咋训练
- B高V低,整体训练差,但是模型较平均,是欠拟合
- B低V高,整体训练好,但是模型偏差很大,过拟合了
### FP/TP/FN/TN以及Precision/Recall
- FP 错误的估计成了正例(本应是负例)
- 其他以此类推
> `$Acc = \frac{TP+TN}{ALL}$`
> `$Pre = \frac{TP}{TP+FP}$` 真实正例占预测正例之比
> `$Recall = \frac{TP}{TP+FN}$` 预测正例占真实正例之比
### ROC Curve
- TP/FP的曲线
- 越靠近左上角越好
### L1/L2
