---
title: 机器学习基础
date: 2020-04-14 00:21:43
author: Kevin Feng
tags:
- 机器学习
- 面试
---
本文主要介绍一些传统机器学习与面试相关的概念
<!--more-->
## LR(逻辑回归)
### 特点
- 用于分类问题
- 使用似然估计
- 对数据偏差大的更敏感
### 公式
> `$Y = \sigma (\theta \times X)$`
- 就是矩阵乘法套了个sigmoid()
#### Loss
>  MSE: `$L = \frac{1}{2m}\Sigma_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$`
> CrossEntropy: `$L = -\frac{1}{m}\Sigma(ylog(h))+(1-y)log(1-h))$`

#### 反向传播:
> `$\sigma'(x) = \sigma(x)(1-\sigma(x)) $`
> `$\theta += \alpha \times \Delta \theta $`

## 线性回归
- 最小二乘法
  - $w = (X^TX)^{-1}X^Ty
- 直接求解不用似然估计(有前提条件)
  - $X^TX$是满秩矩阵
  - 转置和求矩阵的逆都是消耗时间和空间的,如果X大的话可能有内存不足的问题
- 牛顿法
- 梯度下降(还是绕不开)

- 对所有数据敏感
## EM(极大似然估计)

## 决策树
- 优点
  - 可解释

### 构造方法

#### ID3 信息增益
决策树在构建的时候每次优先选择最有区分度的方式.
1. 计算训练集的总信息熵(以label概率计算)
   $\Sigma -p(i)*log_2(p(i))$
2. 计算每个特征的信息熵以及信息增益(某一类群(特征角度)的label概率计算)
3. 信息增益就是总熵-单个label的熵
4. 信息增益越大说明这条区分度越高
- 缺点
  - 更偏向于选择多个属性值的特征

#### C4.5 信息增益比
$SplitInfo_A(S) = -\Sigma_{j=1}^{m}\frac{S_j}{S}log_2\frac{S_j}{S}$
1. 信息增益率是ID3中的信息增益除以SplitInfo_A(S)
- 优点
  - 处理离散型以及连续性类型
  - 将连续型离散化处理
  - 有剪枝
  - 能训练缺失数据
- 缺点
  - 计算效率低,有连续值时特别慢
  - 没考虑到条件属性间的相关性,每次只计算一个条件信息
2. 剪枝
   1. pre剪枝
        预先设定树的高度,缺点:不好确定什么时候应该是最好的
   2. 后剪枝
      1. 根据置信度判断,对不合格的进行减值
#### Cart
- 特点
  - 一定是二叉树
  - 能处理连续型变量
- Gini
  - $Gini(A) = 1-\Sigma_{i=1}^{C}p_i^2$
- 对于离散型只看是不是
- 对所有特征求GINI,选择最低的那个
- 如果是连续型变量,就连续分类看什么事后Gini最低
## K-NN
- 选K个点作为邻居
- 他们投票作为结果
- 计算距离
- 小规模样本ok
- 大规模的计算就太麻烦了
- 不需要迭代
- 跟K-Means没关系
- 
## K-Means
- 聚类问题
- 跟K-NN没关系
- 随机选K个点先聚类然后再重新计算这些点,点是分类之后的点得中点
- 受初始化程度影响
- 多次重复效果更好

## 集成学习
### 特点
通过几个弱分类器组合成强分类器
### Stacking
- 组合不同种的分类器,外加嵌套一层分类器调整不同分类器的权重.
### Bagging
- 将训练集拆成几个子集,分别训练(又放回,可重复,但是抽样多次所以还是不同的子集)
- 最后投票
- 例子: 随机森林
- 可并行计算
### Boosting
- 将弱分类器通过加权组合成强分类器
- 不可并行计算,需要利用上一次的结果,类似RNN
#### AdaBoost
#### XGBoost

## 朴素贝叶斯
> `$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$`

## 算法之外的概念
### Bias/Variance
- Bias体现了模型整体情况,
- variance体现了模型在不同训练集中泛化能力
- B低V低,好模型
- B高V高,没咋训练
- B高V低,整体训练差,但是模型较平均,是欠拟合
- B低V高,整体训练好,但是模型偏差很大,过拟合了
### FP/TP/FN/TN以及Precision/Recall
- FP 错误的估计成了正例(本应是负例)
- 其他以此类推
> `$Acc = \frac{TP+TN}{ALL}$`
> `$Pre = \frac{TP}{TP+FP}$` 真实正例占预测正例之比
> `$Recall = \frac{TP}{TP+FN}$` 预测正例占真实正例之比
### ROC Curve
- TP/FP的曲线
- 越靠近左上角越好
### L1/L2
- L1 = Lasso
- L2 = Ridge
